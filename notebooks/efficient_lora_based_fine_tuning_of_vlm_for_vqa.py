# -*- coding: utf-8 -*-
"""Efficient LoRA-based Fine-Tuning of VLM for VQA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iMSq2_O3sNkEOsGnFoyk_Yu7R6YdeXms
"""

# Install dependencies
!pip install torch torchvision transformers peft bitsandbytes datasets opencv-python pillow --quiet

# Imports
from transformers import Blip2ForConditionalGeneration, Blip2Processor, BitsAndBytesConfig
from peft import get_peft_model, LoraConfig
import torch
from PIL import Image
import requests

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load BLIP-2 with 4-bit quantization config for low memory usage
bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)

model_name = "Salesforce/blip2-opt-2.7b"
processor = Blip2Processor.from_pretrained(model_name)
model = Blip2ForConditionalGeneration.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
)

# Apply LoRA for parameter-efficient fine-tuning
lora_config = LoraConfig(
    r=8,
    lora_alpha=8,
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj"]  # specific to OPT architecture in BLIP-2
)
model = get_peft_model(model, lora_config)

# Confirm trainable parameters
model.print_trainable_parameters()

# Move model to device (GPU/CPU)
model.to(device)

from IPython.display import display
# Demo: Load an example image and question
img_url = "https://fastly.picsum.photos/id/11/200/200.jpg?hmac=LBGO0uEpEmAVS8NeUXMqxcIdHGIcu0JiOb5DJr4mtUI"
image = Image.open(requests.get(img_url, stream=True).raw).convert("RGB")
question = "Describe the scene."

# Preprocess inputs using BLIP-2 processor
prompt = f"Question: {question} Answer:"
inputs = processor(image, text=prompt, return_tensors="pt").to(device)

# Generate answer (inference)
with torch.no_grad():
    generated_ids = model.generate(**inputs, max_new_tokens=20)
    answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()

# Print results
display(image)
print("Question:", question)
print("Answer:", answer)

# Optional: Save LoRA adapters after fine-tuning (uncomment if applicable)
# model.save_pretrained("./lora_finetuned_blip2")

from google.colab import files


print("Upload an image file for VQA inference:")
uploaded = files.upload()

# Load the uploaded image (take the first uploaded file)
image_path = next(iter(uploaded))
image = Image.open(image_path).convert("RGB")
small_image=image.resize((200,200))
display(small_image)

for i in range(0,2):
  # Interactive question input
  question = input("Enter your question about the image: ")
  prompt = f"Question: {question} Answer:"

  # Preprocess inputs and run inference
  inputs = processor(image, text=prompt, return_tensors="pt").to(device)
  with torch.no_grad():
      generated_ids = model.generate(**inputs, max_new_tokens=20)
      answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()

  print("\nQuestion:", question)
  print("Answer:", answer)

# Interactive question input
question = input("Enter your question about the image: ")
prompt = f"Question: {question} Answer:"

# Preprocess inputs and run inference
inputs = processor(image, text=prompt, return_tensors="pt").to(device)
with torch.no_grad():
    generated_ids = model.generate(**inputs, max_new_tokens=20)
    answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()

print("\nQuestion:", question)
print("Answer:", answer)

